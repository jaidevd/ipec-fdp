{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import make_classification, perceptron_decision_boundary, sigmoid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from theano import tensor as T\n",
    "from theano import function, shared\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('figure', figsize=(8, 6))\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation of a logistic neuron:\n",
    "\n",
    "## $$ z = \\sum_{i \\in L} x_{i}w_{i} + b$$ \n",
    "\n",
    "## Predicted output:\n",
    "\n",
    "## $$ y = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "## Loss function: Mean Squared Error:\n",
    "## $$ E = \\frac{1}{2}\\sum_{i \\in T} (t^{i} - y^{i})^{2} $$\n",
    "## Where $T$ is the set of training cases, and $t$ is the target value\n",
    "\n",
    "# Logistic Neuron in NumPy:\n",
    "\n",
    "## Step 1: Make dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_classification()\n",
    "W = np.random.rand(2, 1)\n",
    "B = np.random.rand(1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_decision_boundary(W.ravel().tolist() + [B[0]], X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Get activation and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# activation\n",
    "Z = np.dot(X, W) + B\n",
    "\n",
    "# prediction\n",
    "Y_pred = sigmoid(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Derive gradient for loss function\n",
    "## Gradient:  $\\nabla{E} = \\frac{\\partial{E}}{\\partial{w_{j}}}$\n",
    "\n",
    "## Trick:\n",
    "## $$\n",
    "\\begin{equation}\n",
    "            \\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{W}}} = \\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W}}}\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{y}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "## Second term on RHS:\n",
    "## $$\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{y}}} = -(\\mathbf{t} - \\mathbf{y})$$\n",
    "\n",
    "## First term on RHS: (using same trick):\n",
    "## $$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W}}} = \\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{z}}}\\frac{\\partial{\\mathbf{z}}}{\\partial{\\mathbf{W}}}$$\n",
    "\n",
    "## From first exercise (Not available!), first term on RHS reduces to:\n",
    "## $$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{z}}} = \\mathbf{y}(1 - \\mathbf{y})$$\n",
    "\n",
    "## From definition of logistic activation:\n",
    "## $$\\mathbf{z} = \\mathbf{X}\\mathbf{W} + \\mathbf{b} $$\n",
    "\n",
    "## Second term in RHS:\n",
    "## $$\\frac{\\partial{\\mathbf{z}}}{\\partial{\\mathbf{W}}} = \\mathbf{X}$$\n",
    "\n",
    "## Substituting:\n",
    "## $$\\frac{\\partial{\\mathbf{y}}}{\\partial{\\mathbf{W}}} = \\mathbf{y}(1 - \\mathbf{y})\\mathbf{X}$$\n",
    "\n",
    "## Substituting back in original equation\n",
    "## $$\\frac{\\partial{\\mathbf{E}}}{\\partial{\\mathbf{W}}} = -(\\mathbf{t} - \\mathbf{y})\\mathbf{y}(1 - \\mathbf{y})\\mathbf{X}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using this gradient to train neuron with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, weights, bias=None):\n",
    "    if bias is not None:\n",
    "        z = np.dot(X, weights) + bias\n",
    "    else:\n",
    "        z = np.dot(X, weights)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def train(X, Y, weights, alpha=0.3):\n",
    "    y_hat = predict(X, weights)\n",
    "    _gw = -1 * (Y - y_hat) * y_hat * (1 - y_hat)\n",
    "    _gw = np.repeat(_gw, X.shape[1], axis=1)\n",
    "    weights -= (alpha * _gw * X).sum(0).reshape(-1, 1)\n",
    "    return weights\n",
    "\n",
    "def loss(y1, y2):\n",
    "    return (0.5 * ((y1 - y2) ** 2)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    y_hat = predict(X, W)\n",
    "    W = train(X, Y, W)\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Loss: \", loss(Y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_decision_boundary(W.ravel().tolist() + [B[0]], X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Implement logistic neuron with Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enter code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
